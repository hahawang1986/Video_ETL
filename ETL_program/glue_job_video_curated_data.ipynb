{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"# AWS Glue Studio Notebook\n",
				"##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"#### Optional: Run this cell to see available notebook commands (\"magics\").\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"%help"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"####  Run this cell to set up and start your interactive session.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"%idle_timeout 60\n",
				"%glue_version 4.0\n",
				"%worker_type G.1X\n",
				"%number_of_workers 2\n",
				"\n",
				"import sys\n",
				"from awsglue.transforms import *\n",
				"from awsglue.utils import getResolvedOptions\n",
				"from pyspark.context import SparkContext\n",
				"from awsglue.context import GlueContext\n",
				"from awsglue.job import Job\n",
				"  \n",
				"sc = SparkContext.getOrCreate()\n",
				"glueContext = GlueContext(sc)\n",
				"spark = glueContext.spark_session\n",
				"job = Job(glueContext)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 6,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"root\n",
						"|-- DateTime: string\n",
						"|-- VideoTitle: string\n",
						"|-- events: string\n",
						"|-- id: string\n",
						"\n",
						"995676\n"
					]
				}
			],
			"source": [
				"from datetime import *\n",
				"current_date = datetime.today().strftime('%Y-%m-%d')\n",
				"\n",
				"s3_path = f's3://video-raw-bucket/{current_date}/'\n",
				"\n",
				"dynamic_frame = glueContext.create_dynamic_frame.from_options(\n",
				"    connection_type=\"s3\",\n",
				"    connection_options={\"paths\": [s3_path]},\n",
				"    format=\"csv\",\n",
				"    format_options={\"withHeader\": True}#,\"optimizePerformance\": True}\n",
				")\n",
				"dynamic_frame.printSchema()\n",
				"dynamic_frame.count()\n",
				"df = dynamic_frame.toDF()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Layout Check"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 7,
			"metadata": {
				"tags": [],
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"\n"
					]
				}
			],
			"source": [
				"\n",
				"df = df.withColumn(\"VideoTitle\", trim(col(\"VideoTitle\")))\n",
				"# 替换 VideoTitle 列中每个 | 符号后的空格\n",
				"df = df.withColumn(\"VideoTitle\", regexp_replace(col(\"VideoTitle\"), r'\\|\\s+', '|'))\\\n",
				"       .withColumn(\"VideoTitle_split\", split(col(\"VideoTitle\"), \"\\\\|\")).\\\n",
				"       .filter((((size(col(\"VideoTitle_split\")) == 2) & (col(\"VideoTitle_split\")[0] == \"news\")) |\n",
				"    ((size(col(\"VideoTitle_split\")) == 4) & (col(\"VideoTitle_split\")[0] != \"news\"))))\\\n",
				"       .drop(\"VideoTitle_split\")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Garbled characters or unreadable character"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "plaintext"
				}
			},
			"outputs": [],
			"source": [
				"from pyspark.sql.functions import size,lit\n",
				"\n",
				"# None value\n",
				"# When you call df.filter(condition), Spark applies this condition to each row of data.\n",
				"# If any column value in a row meets the criteria specified in the condition (i.e., is null), \n",
				"# then that row will be included in anomalies_df.\n",
				"condition = None\n",
				"for column in df.columns:\n",
				"    #print(column)\n",
				"    if condition is None:\n",
				"        condition = col(column).isNull()\n",
				"    else:\n",
				"        condition = condition | col(column).isNull()\n",
				"\n",
				"# anomalies_df = df.filter(condition) \n",
				"df = df.filter(~condition)\n",
				"df.count()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## outliers"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "plaintext"
				}
			},
			"outputs": [],
			"source": [
				"from pyspark.sql.functions import to_date,length\n",
				"from pyspark.sql.types import StringType, ArrayType\n",
				"date_cols = [\"DateTime\", \"VideoTitle\",\"events\"]\n",
				"max_length = 150\n",
				"for col_name in date_cols:\n",
				"    if col_name == \"DateTime\":\n",
				"        df = df.withColumn(\"DateTime_test\", to_date(col_name, \"yyyy-MM-dd HH:mm:ss\"))\\\n",
				"        .filter(col(\"DateTime_test\").isNotNull())\\\n",
				"        .drop(\"DateTime_test\") #if df_fault == 0 then df = df\n",
				"        print(\"the number of fault DateTime is:\",df.count())\n",
				"    elif col_name == \"VideoTitle\":\n",
				"        df = df.filter(length(col(col_name)) <= max_length)\n",
				"        print(\"the number of fault VideoTitle is:\"df.count())\n",
				"    elif col_name == \"events\":\n",
				"        df = df.filter(~df.events.contains(\"206\"))\n",
				"        print(\"the number of fault events is:\"df.count())\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Build Data Warehouse"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## DimDate"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "plaintext"
				}
			},
			"outputs": [],
			"source": [
				"from pyspark.sql.functions import col, hour, minute, dayofweek, month, quarter, year, to_timestamp\n",
				"\n",
				"df_time = df\\\n",
				"    .withColumn(\"DateTime\", to_timestamp(col(\"DateTime\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
				"    .withColumn(\"Hour\", hour(\"DateTime\")) \\\n",
				"    .withColumn(\"Minute\", minute(\"DateTime\")) \\\n",
				"    .withColumn(\"DayOfWeek\", dayofweek(\"DateTime\")) \\\n",
				"    .withColumn(\"Month\", month(\"DateTime\")) \\\n",
				"    .withColumn(\"Quarter\", quarter(\"DateTime\")) \\\n",
				"    .withColumn(\"Year\", year(\"DateTime\"))\n",
				"\n",
				"from pyspark.sql.functions import concat, lit, row_number\n",
				"from pyspark.sql.window import Window\n",
				"df_time = df_time.dropDuplicates([\"DateTime\"])  # maybe several events coincide\n",
				"# Generate a unique DateKey\n",
				"windowSpec = Window.orderBy(\"DateTime\")\n",
				"df_time = df_time.withColumn(\"DateKey\", row_number().over(windowSpec))\n",
				"\n",
				"# Select required columns for DimDate\n",
				"DimDate = df_time.select(\"DateKey\", \"DateTime\", \"Hour\", \"Minute\", \"DayOfWeek\", \"Month\", \"Quarter\", \"Year\")\n",
				"\n",
				"# Show the prepared DimDate DataFrame\n",
				"DimDate.show(2,truncate=False)\n",
				"DimDate.count()\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Build DimPlatform, DimVideotype, DimVideoname"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "plaintext"
				}
			},
			"outputs": [],
			"source": [
				"from pyspark.sql.functions import split, col\n",
				"\n",
				"# 根据拆分后的结果创建新列 DimPlatform, DimVideoType, DimVideoName, DimvideoTopic\n",
				"df_video = df\\\n",
				"    .withColumn(\"DimPlatform\", col(\"VideoTitle_split\")[0]) \\\n",
				"    .withColumn(\"DimVideoType\", when(col(\"VideoTitle_split\")[0] == \"news\", \"Unknow\").otherwise(col(\"VideoTitle_split\")[1])) \\\n",
				"    .withColumn(\"DimVideoName\", when(col(\"VideoTitle_split\")[0] == \"news\", \"Unknow\").otherwise(col(\"VideoTitle_split\")[2])) \\\n",
				"    .withColumn(\"DimvideoTopic\", when(col(\"VideoTitle_split\")[0] == \"news\", col(\"VideoTitle_split\")[1]).otherwise(col(\"VideoTitle_split\")[3]))\n",
				"df_video = df_video.withColumn(\"DimVideoType\",\n",
				"                               when(col(\"DimVideoType\") == \"Clips\", \"Clip\")\n",
				"                               .when(col(\"DimVideoType\") == \"Episodes\", \"Episode\")\n",
				"                               .otherwise(col(\"DimVideoType\")))\n",
				"df_video.show(1)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "plaintext"
				}
			},
			"outputs": [],
			"source": [
				"df_platform = df_video.select(\"DimPlatform\").distinct()\n",
				"\n",
				"windowSpec = Window.orderBy(\"DimPlatform\")\n",
				"df_platform = df_platform.withColumn(\"platform_id\", row_number().over(windowSpec))\n",
				"\n",
				"# Select required columns for DimDate\n",
				"DimPlatform = df_platform.select(\"platform_id\", \"DimPlatform\") \\\n",
				"                        .withColumnRenamed(\"DimPlatform\",\"platform\")\n",
				"\n",
				"# Show the prepared DimDate DataFrame\n",
				"DimPlatform.show(truncate=False)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "plaintext"
				}
			},
			"outputs": [],
			"source": [
				"from pyspark.sql.functions import asc\n",
				"windowSpec = Window.orderBy(asc(\"DimVideoType\"))\n",
				"df_videotype = df_video.select(\"DimVideoType\")\\\n",
				"                        .distinct()\\\n",
				"                        .withColumn(\"videotype_id\", row_number().over(windowSpec))\n",
				"\n",
				"# Select required columns for DimDate\n",
				"DimVideoType = df_videotype.select(\"videotype_id\", \"DimVideoType\") \\\n",
				"                        .withColumnRenamed(\"DimVideoType\",\"videotype\")\n",
				"\n",
				"# Show the prepared DimDate DataFrame\n",
				"DimVideoType.show(truncate=False)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "plaintext"
				}
			},
			"outputs": [],
			"source": [
				"windowSpec = Window.orderBy(asc(\"DimVideoName\"))\n",
				"df_videoname = df_video.select(\"DimVideoName\")\\\n",
				"                        .distinct()\\\n",
				"                        .withColumn(\"videoname_id\", row_number().over(windowSpec))\n",
				"\n",
				"# Select required columns for DimDate\n",
				"DimVideoName = df_videoname.select(\"videoname_id\", \"DimVideoName\") \\\n",
				"                        .withColumnRenamed(\"DimVideoName\",\"videoname\")\n",
				"\n",
				"# Show the prepared DimDate DataFrame\n",
				"DimVideoName.show(5,truncate=False)\n",
				"\n",
				"DimVideoName.count()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "plaintext"
				}
			},
			"outputs": [],
			"source": [
				"windowSpec = Window.orderBy(asc(\"DimvideoTopic\"))\n",
				"df_topic = df_video.dropDuplicates([\"DimvideoTopic\"])\\\n",
				"                    .withColumn(\"topic_id\", row_number().over(windowSpec))\\\n",
				"                    .select(\"topic_id\",\"DimvideoTopic\",\"DimVideoName\",\"DimVideoType\",\"DimPlatform\")\\\n",
				"                    .withColumnRenamed(\"DimvideoTopic\",\"videoTopic\")\\\n",
				"                    .withColumnRenamed(\"DimVideoName\",\"videoName\")\\\n",
				"                    .withColumnRenamed(\"DimVideoType\",\"videoType\")\\\n",
				"                    .withColumnRenamed(\"DimPlatform\",\"videoPlatform\")\n",
				"df_topic.show(3,truncate=False)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "plaintext"
				}
			},
			"outputs": [],
			"source": [
				"df_topic = df_topic.join(DimVideoName, df_topic.videoName == DimVideoName.videoname, \"left\") \\\n",
				"                 .join(DimVideoType, df_topic.videoType == DimVideoType.videotype, \"left\") \\\n",
				"                 .join(DimPlatform, df_topic.videoPlatform == DimPlatform.platform, \"left\")\n",
				"\n",
				"df_topic.show(2,truncate=False)\n",
				"DimVideoTopic = df_topic.select(\"topic_id\",\"videoTopic\",\"videoname_id\",\"videotype_id\",\"platform_id\")\n",
				"DimVideoTopic.show(2)\n",
				"DimVideoTopic.count()"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"## Fact Table"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "plaintext"
				}
			},
			"outputs": [],
			"source": [
				"windowSpec = Window.orderBy(asc(\"DateTime\"))\n",
				"FactVideo = df_video.join(df_topic,df_video.DimvideoTopic == df_topic.videoTopic,\"left\")\\\n",
				"                    .withColumn(\"record_id\", row_number().over(windowSpec))\\\n",
				"                    .withColumnRenamed(\"id\",\"user_id\")\\\n",
				"                    .select(\"record_id\",\"user_id\",\"DateTime\",\"videoTopic\",\"events\")\n",
				"            \n",
				"FactVideo.show(10)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"vscode": {
					"languageId": "plaintext"
				}
			},
			"outputs": [],
			"source": [
				"\n",
				"date_today = datetime.now().strftime(\"%Y-%m-%d\")\n",
				"output_path1 = f's3://video-curated-bucket/data-warehouse/dimdate/{date_today}'\n",
				"output_path2 = f's3://video-curated-bucket/data-warehouse/dimplatform/{date_today}'\n",
				"output_path3 = f's3://video-curated-bucket/data-warehouse/dimvideotype/{date_today}'\n",
				"output_path4 = f's3://video-curated-bucket/data-warehouse/dimvideoname/{date_today}'\n",
				"output_path5 = f's3://video-curated-bucket/data-warehouse/dimvideotopic/{date_today}'\n",
				"output_path6 = f's3://video-curated-bucket/data-warehouse/factvideo/{date_today}'\n",
				"\n",
				"\n",
				"FactVideo.write \\\n",
				"    .mode('overwrite') \\\n",
				"    .option(\"header\", \"true\") \\\n",
				"    .csv(output_path6)\n",
				"\n",
				"DimPlatform.coalesce(1).write \\\n",
				"    .mode('overwrite') \\\n",
				"    .option(\"header\", \"true\") \\\n",
				"    .csv(output_path2)\n",
				"\n",
				"DimDate.write \\\n",
				"    .mode('overwrite') \\\n",
				"    .option(\"header\", \"true\") \\\n",
				"    .csv(output_path1)\n",
				"\n",
				"DimVideoType.coalesce(1).write\n",
				"    .mode('overwrite') \\\n",
				"    .option(\"header\", \"true\") \\\n",
				"    .csv(output_path3)\n",
				"\n",
				"DimVideoName.coalesce(1).write\\\n",
				"    .mode('overwrite') \\\n",
				"    .option(\"header\", \"true\") \\\n",
				"    .csv(output_path4)\n",
				"\n",
				"DimVideoTopic.write\\\n",
				"    .mode('overwrite') \\\n",
				"    .option(\"header\", \"true\") \\\n",
				"    .csv(output_path5)"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "Python_Glue_Session",
			"pygments_lexer": "python3"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
